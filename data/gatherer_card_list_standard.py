# -*- coding: utf-8 -*-
"""gatherer_card_list.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DXKUYk_QdB3rXxZUx4vK3p35ELcZDX9Y

This script will parse gatherer.wizards.com for all cards in the current standard format. When running this, please be patient as the response time for a gatherer page is rather long, so each call to the page takes about 36 seconds. 

Currently, this script will support up to 30 gatherer pages worth of content. Beyond that and the collection makes the rest of the prediction script take very long, so if this were to be extended for Modern or Legacy format, we would need a way to parse further pages and speed up the per page response time.
"""

from bs4 import BeautifulSoup
from collections import OrderedDict
import requests
import json
import re

def get_new_page(url,target_page_num):
  url_split = re.split('\?|&', url)
  temp_string = 'page=' + target_page_num
  url_split[1] = temp_string
  url_next = url_split[0] + '?' + url_split[1]
  for arg in url_split[2:]:
    url_next = url_next + '&' + arg
  return url_next

# we need to get all the content from the sets, which are paginated
def get_page_numbers(soupObj):
  #so find the page selector div
  pageFinder = soupObj.find("div", {"id": "ctl00_ctl00_ctl00_MainContent_SubContent_topPagingControlsContainer"})
  page_nums = []
  #and parse it to get all of the page numbers for the first 15 pages
  tags = pageFinder.find_all('a')
  for row in tags:
    if row.get_text() != '>' and row.get_text() != '>>' and row.get_text() != '<' and row.get_text() != '<<':
      page_nums.append(row.get_text())
  #pop the end of page list character in the text ( << and >> are special scharacter and must be dropped seperately)
  page_nums.pop()
  #if the first element in the list is not page number 1, drop it as it will be a special character
  if page_nums[0] != '1':
    page_nums.pop(0)
  return page_nums

# base URl
url="https://gatherer.wizards.com/Pages/Search/Default.aspx?page=0&output=compact&action=advanced&format=+[%22Standard%22]"

# gather the content
html_content = requests.get(url).text

# Parse the html content into a soup object
soup = BeautifulSoup(html_content, "lxml")

#we need the complete set of pages in the given format
#so lets start by calling our page number function with the given root defined above (page 0)
page_numbers = get_page_numbers(soup)

#we start by navigating to the last page in our current index
last_page_url = get_new_page(url,page_nums[len(page_nums) - 1])
# gather the new content
html_content_new = requests.get(last_page_url).text

# Parse the html content into a soup object
soup_next = BeautifulSoup(html_content_new, "lxml")

#gather the new page numbers
page_numbers_next = get_page_numbers(soup_next)

#combine the two lists
pages = page_numbers + page_numbers_next

#and now we eliminate the duplicates
pages = list(OrderedDict.fromkeys(pages))

cardsDict = {}
print(pages)
for page in pages:
  # gather the content
  html_content = requests.get(url).text

  # Parse the html content into a soup object
  soup = BeautifulSoup(html_content, "lxml") 

  # we only want table data sound in the body, specifically card names denoted with the class 'name top'
  print(page)
  print(url)
  body = soup.body
  tablerows = body.find_all("td", {"class": "name top"})

  # for each row in the table, write the data out to a dictionary. Because this is an ideal dataset
  # we assume a full set (each card has 4 copies avalible)
  for row in tablerows:
    atags = row.find('a')
    if atags:
      print(atags.get_text())
      cardsDict[atags.get_text()] = 4 
  #now we change the page number and get the next set of context
  page_next = get_new_page(url,page)
  url = page_next
  print('Next Page URL: %s' % url)

#with our dictionary, we can now output to a json.
json_obj = json.dumps(cardsDict,indent=4)
print(json_obj)
with open('collection.json', 'w') as outfile:
    outfile.write(json_obj)